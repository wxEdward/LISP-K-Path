{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "from torch_geometric.loader import DataLoader,RandomNodeSampler, NeighborLoader, NeighborSampler\n",
    "from torch_geometric.utils import structured_negative_sampling, to_dense_adj, erdos_renyi_graph\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading biological graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edges_and_feature(edge_list_file, node_features_file):\n",
    "    edges = np.load(edge_list_file)\n",
    "    assert edges.shape[1] == 2\n",
    "    features = np.load(node_features_file)\n",
    "    n_nodes = features.shape[0]\n",
    "    assert edges.min() >= 0\n",
    "    assert edges.max() <= n_nodes-1 \n",
    "\n",
    "    print(f\"Number of edges: {edges.shape[0]}, number of nodes: {n_nodes}, feature dim: {features.shape[1]}\")\n",
    "    \n",
    "    data = Data(x = torch.from_numpy(features).float(), edge_index=torch.from_numpy(edges).t().contiguous())\n",
    "    return data\n",
    "\n",
    "def load_edges(edge_list_file):\n",
    "    edges = np.load(edge_list_file)\n",
    "    assert edges.shape[1] == 2\n",
    "    print(f\"Number of edges: {edges.shape[0]}, min node index: {edges.min()}, min node index: {edges.max()}\")\n",
    "\n",
    "    data = Data(edge_index=torch.from_numpy(edges).t().contiguous())\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 8959, number of nodes: 1870, feature dim: 128\n",
      "Number of edges: 8959, min node index: 0, min node index: 1869\n"
     ]
    }
   ],
   "source": [
    "data = load_edges_and_feature(\n",
    "    \"data/bio-yeast-protein/bio-yeast-protein-inter.edges_edge_list_mapped.npy\", \n",
    "    \"data/bio-yeast-protein/bio-yeast-protein-inter.edges.embeddings_features.npy\")\n",
    "data = data.to(device)\n",
    "\n",
    "data_no_feat = load_edges(\"data/bio-yeast-protein/bio-yeast-protein-inter.edges_edge_list_mapped.npy\")\n",
    "data_no_feat = data_no_feat.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate DeepWalk features\n",
    "\n",
    "https://github.com/phanein/deepwalk\n",
    "\n",
    "https://github.com/shenweichen/GraphEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generative_graph_deepwalk(num_nodes, edge_index):\n",
    "#     # er_graph = erdos_renyi_graph(num_nodes=256, edge_prob=0.1, directed=False)\n",
    "#     er_graph = edge_index\n",
    "#     model = Node2Vec(er_graph, embedding_dim=128, walk_length=10,\n",
    "#                      context_size=10, walks_per_node=10,\n",
    "#                      num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "#     loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "#     optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    \n",
    "#     for epoch in range(1, 101):\n",
    "#         model.train()\n",
    "#         total_loss = 0\n",
    "#         for pos_rw, neg_rw in loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         train_loss = total_loss / len(loader)\n",
    "#         print(f'Epoch: {epoch:02d}, Loss: {train_loss:.4f}')\n",
    "#     z = model(torch.arange(num_nodes, device=device))\n",
    "#     return Data(x=z, edge_index=er_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1870, 128], edge_index=[2, 8959])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directly use loaded feature\n",
    "# TODO: Get deepwalk features on the fly\n",
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train GNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "feat_dim = data.x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1870, 5])\n"
     ]
    }
   ],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(feat_dim, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "\n",
    "        self.lin1 = nn.Linear(64, 32)\n",
    "        self.lin2 = nn.Linear(32, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "def test_model():\n",
    "    test_net = Network(128, k_colors).to(device)\n",
    "    test_out = test_net(data.x, data.edge_index)\n",
    "    print(test_out.shape)\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_product_loss(network_out):\n",
    "    batch_size = network_out.shape[0]//2\n",
    "    a = network_out[:batch_size, :]\n",
    "    b = network_out[batch_size:, :]\n",
    "    mean_dot_prod = torch.mean(torch.sum(a * b, dim=1), dim=0)\n",
    "    # print(b.argmax(dim=1).shape)\n",
    "    # same_color = (a.argmax(dim=1) == b.argmax(dim=1)).int().sum()\n",
    "    # return same_color/a.shape[0]\n",
    "    return mean_dot_prod\n",
    "\n",
    "\n",
    "def negative_entropy_loss(network_out):\n",
    "    # num_nodes x k\n",
    "    log = torch.log(network_out)\n",
    "    prod = network_out * log \n",
    "    s = torch.sum(prod, dim=-1)\n",
    "    return s.mean()\n",
    "\n",
    "\n",
    "def percent_same_color(network_out):\n",
    "    # percent of pairs having same color\n",
    "    # [2 * batch_size, feature_size]\n",
    "    arg_max = torch.argmax(network_out, dim=1)\n",
    "    arg_max_reshaped = arg_max.reshape([2, -1])\n",
    "    result = torch.sum((arg_max_reshaped[0,:] == arg_max_reshaped[1,:]).int())/arg_max_reshaped.shape[1]\n",
    "    # print(result.shape)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def assign_color_sampled_edges(sampled_edges, k):\n",
    "    # given a sampled batch of edges, assign random color to each end points\n",
    "    all_nodes = sampled_edges.ravel()\n",
    "    # print(sampled_edges.shape)\n",
    "    # colors = torch.randint(low=0, high=k, size=all_nodes.shape)\n",
    "    color_dict = {node.item():torch.randint(low=0, high=k, size=(1,)).item() for node in all_nodes}\n",
    "    count_same = 0\n",
    "    for i in range(sampled_edges.shape[1]):\n",
    "        assert (sampled_edges[0,i] in all_nodes)\n",
    "        assert (sampled_edges[1,i] in all_nodes)\n",
    "        if color_dict[sampled_edges[0,i].item()] == color_dict[sampled_edges[1,i].item()]:\n",
    "            count_same+=1\n",
    "    return count_same/sampled_edges.shape[1]\n",
    "\n",
    "\n",
    "def cosine_sim(node_probs, edge_index): \n",
    "    # assuming continous node numbering\n",
    "    # for each edge, get a loss term, then take mean\n",
    "    total_loss = 0.\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1_prob = node_probs[edge_index[0, i],:]\n",
    "        node2_prob = node_probs[edge_index[1, i],:]\n",
    "        total_loss += torch.dot(node1_prob, node2_prob)\n",
    "    return total_loss/edge_index.shape[1]\n",
    "\n",
    "\n",
    "def same_color_prob(node_probs, edge_index):\n",
    "    n_same_color = 0\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1_prob = node_probs[edge_index[0, i],:]\n",
    "        node2_prob = node_probs[edge_index[1, i],:]\n",
    "        # print(torch.argmax(node1_prob), torch.argmax(node2_prob))\n",
    "        if torch.argmax(node1_prob) == torch.argmax(node2_prob):\n",
    "            # print(node1_prob, node2_prob)\n",
    "            n_same_color+=1\n",
    "    return n_same_color/edge_index.shape[1]\n",
    "\n",
    "\n",
    "def mse_loss(node_probs, edge_index):\n",
    "    total_loss = 0.\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1_prob = node_probs[edge_index[0, i],:]\n",
    "        node2_prob = node_probs[edge_index[1, i],:]\n",
    "        total_loss += mse_criterion(node1_prob, node2_prob)\n",
    "    return total_loss/edge_index.shape[1]\n",
    "\n",
    "\n",
    "def ncut_loss(node_probs, edge_index):\n",
    "    Y = node_probs \n",
    "    A = torch.squeeze(to_dense_adj(edge_index, max_num_nodes=node_probs.shape[0]))\n",
    "    # print(A.shape, node_probs.shape)\n",
    "    assert (A.shape[0] == node_probs.shape[0])\n",
    "    \n",
    "    D = torch.sum(A, dim=1)\n",
    "    Gamma = Y.t()@D \n",
    "    sum_mat = torch.mul(torch.div(Y, Gamma)@((1-Y).t()), A)\n",
    "    # sum_mat = torch.mul(Y@((1-Y).t()), A)\n",
    "    # print(sum_mat)\n",
    "    ncut = torch.sum(sum_mat)\n",
    "\n",
    "    return ncut\n",
    "\n",
    "\n",
    "def test_ncut_loss():\n",
    "    # edge_index=torch.tensor([[0,1], [1, 0], [0, 3], [3, 0], [1, 2], [2, 1], [2, 3], [3, 2]])\n",
    "    edge_index = torch.tensor([[0, 1, 0, 3, 1, 2, 2, 3],[1, 0, 3, 0, 2, 1, 3, 2]])\n",
    "    # A = torch.tensor([\n",
    "    #     [0, 1, 0, 1],\n",
    "    #     [1, 0, 1, 0],\n",
    "    #     [0, 1, 0, 1],\n",
    "    #     [1, 0, 1, 0]\n",
    "    # ])\n",
    "    # print(edge_index)\n",
    "\n",
    "    A = torch.squeeze(to_dense_adj(edge_index, max_num_nodes=4))\n",
    "    # print(A)\n",
    "    Y = torch.tensor([\n",
    "        [0.5, 0.5],\n",
    "        [0.25, 0.75],\n",
    "        [0.75, 0.25],\n",
    "        [0.1, 0.9]\n",
    "    ])\n",
    "\n",
    "    D = torch.tensor([2., 2., 2., 2.])\n",
    "\n",
    "    Gamma = Y.t()@D\n",
    "    loss_matrix = ((Y/Gamma)@((1-Y).t()))*A\n",
    "    # loss_matrix = ((Y)@((1-Y).t()))*A\n",
    "    # print(loss_matrix)\n",
    "    correct_loss = loss_matrix.sum()\n",
    "\n",
    "    # assert (ncut_loss(Y, edge_index) == correct_loss)\n",
    "\n",
    "\n",
    "def size_reg(node_probs, k_colors):\n",
    "    sizes = node_probs.sum(dim=1)\n",
    "    mean_size = node_probs.shape[0]/k_colors\n",
    "    size_reg = torch.dot(sizes-mean_size, sizes-mean_size)\n",
    "    return size_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_colors = 4\n",
    "model = Network(data.x.shape[1], k_colors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = NeighborLoader(data, num_neighbors=[10] * 2, shuffle=True, batch_size=10)\n",
    "lr = 0.001\n",
    "weight_decay = 5e-4\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "cos_sim = torch.nn.CosineSimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: -3.4324069023132324, same color prob 0.18293415009975433\n",
      "iteration: 1 loss: -3.4294562339782715, same color prob 0.20677298307418823\n",
      "iteration: 2 loss: -3.458091974258423, same color prob 0.22450792789459229\n",
      "iteration: 3 loss: -3.433349609375, same color prob 0.20599651336669922\n",
      "iteration: 4 loss: -3.4459166526794434, same color prob 0.18340717256069183\n",
      "iteration: 5 loss: -3.4536495208740234, same color prob 0.17122671008110046\n",
      "iteration: 6 loss: -3.4480795860290527, same color prob 0.167527973651886\n",
      "iteration: 7 loss: -3.459488868713379, same color prob 0.16992127895355225\n",
      "iteration: 8 loss: -3.4509153366088867, same color prob 0.18261969089508057\n",
      "iteration: 9 loss: -3.4428887367248535, same color prob 0.19409187138080597\n",
      "iteration: 10 loss: -3.4625682830810547, same color prob 0.20731747150421143\n",
      "iteration: 11 loss: -3.450997829437256, same color prob 0.2113988697528839\n",
      "iteration: 12 loss: -3.4603829383850098, same color prob 0.2116210162639618\n",
      "iteration: 13 loss: -3.4563424587249756, same color prob 0.1914234757423401\n",
      "iteration: 14 loss: -3.455345869064331, same color prob 0.17164529860019684\n",
      "iteration: 15 loss: -3.4656591415405273, same color prob 0.17436325550079346\n",
      "iteration: 16 loss: -3.4444503784179688, same color prob 0.19163765013217926\n",
      "iteration: 17 loss: -3.4431610107421875, same color prob 0.20127961039543152\n",
      "iteration: 18 loss: -3.44966721534729, same color prob 0.20818385481834412\n",
      "iteration: 19 loss: -3.462348461151123, same color prob 0.19015218317508698\n",
      "iteration: 20 loss: -3.470299005508423, same color prob 0.1705196499824524\n",
      "iteration: 21 loss: -3.4555726051330566, same color prob 0.17090609669685364\n",
      "iteration: 22 loss: -3.4588427543640137, same color prob 0.1819106638431549\n",
      "iteration: 23 loss: -3.4670469760894775, same color prob 0.18756058812141418\n",
      "iteration: 24 loss: -3.450408697128296, same color prob 0.20152463018894196\n",
      "iteration: 25 loss: -3.4593420028686523, same color prob 0.19797955453395844\n",
      "iteration: 26 loss: -3.4672179222106934, same color prob 0.18763868510723114\n",
      "iteration: 27 loss: -3.4600491523742676, same color prob 0.1889413446187973\n",
      "iteration: 28 loss: -3.4569461345672607, same color prob 0.19876253604888916\n",
      "iteration: 29 loss: -3.4635965824127197, same color prob 0.2109803557395935\n",
      "iteration: 30 loss: -3.4559404850006104, same color prob 0.21815146505832672\n",
      "iteration: 31 loss: -3.459031820297241, same color prob 0.195951908826828\n",
      "iteration: 32 loss: -3.454744815826416, same color prob 0.18508753180503845\n",
      "iteration: 33 loss: -3.4774913787841797, same color prob 0.17702265083789825\n",
      "iteration: 34 loss: -3.461871385574341, same color prob 0.17596682906150818\n",
      "iteration: 35 loss: -3.4648866653442383, same color prob 0.177789568901062\n",
      "iteration: 36 loss: -3.4770119190216064, same color prob 0.18393807113170624\n",
      "iteration: 37 loss: -3.466752767562866, same color prob 0.18535226583480835\n",
      "iteration: 38 loss: -3.454775333404541, same color prob 0.18006448447704315\n",
      "iteration: 39 loss: -3.468541383743286, same color prob 0.18570545315742493\n",
      "iteration: 40 loss: -3.4626739025115967, same color prob 0.19901113212108612\n",
      "iteration: 41 loss: -3.4642903804779053, same color prob 0.2077162116765976\n",
      "iteration: 42 loss: -3.4737517833709717, same color prob 0.18831494450569153\n",
      "iteration: 43 loss: -3.467113494873047, same color prob 0.16514527797698975\n",
      "iteration: 44 loss: -3.478832244873047, same color prob 0.1541447639465332\n",
      "iteration: 45 loss: -3.4673309326171875, same color prob 0.1624322235584259\n",
      "iteration: 46 loss: -3.477414608001709, same color prob 0.18985581398010254\n",
      "iteration: 47 loss: -3.457144021987915, same color prob 0.20155709981918335\n",
      "iteration: 48 loss: -3.4594457149505615, same color prob 0.19705207645893097\n",
      "iteration: 49 loss: -3.4606242179870605, same color prob 0.18617677688598633\n",
      "iteration: 50 loss: -3.469972610473633, same color prob 0.17240077257156372\n",
      "iteration: 51 loss: -3.4706664085388184, same color prob 0.170492485165596\n",
      "iteration: 52 loss: -3.4714746475219727, same color prob 0.1807805448770523\n",
      "iteration: 53 loss: -3.45584774017334, same color prob 0.18877877295017242\n",
      "iteration: 54 loss: -3.471724510192871, same color prob 0.19776776432991028\n",
      "iteration: 55 loss: -3.4782462120056152, same color prob 0.19181664288043976\n",
      "iteration: 56 loss: -3.482555389404297, same color prob 0.1793888658285141\n",
      "iteration: 57 loss: -3.4808919429779053, same color prob 0.16609688103199005\n",
      "iteration: 58 loss: -3.4706709384918213, same color prob 0.16755232214927673\n",
      "iteration: 59 loss: -3.464995861053467, same color prob 0.17175765335559845\n",
      "iteration: 60 loss: -3.476565361022949, same color prob 0.19083403050899506\n",
      "iteration: 61 loss: -3.4829373359680176, same color prob 0.2091885507106781\n",
      "iteration: 62 loss: -3.4709272384643555, same color prob 0.19674892723560333\n",
      "iteration: 63 loss: -3.468806505203247, same color prob 0.18040722608566284\n",
      "iteration: 64 loss: -3.4849700927734375, same color prob 0.1743149757385254\n",
      "iteration: 65 loss: -3.46641206741333, same color prob 0.17942053079605103\n",
      "iteration: 66 loss: -3.4803857803344727, same color prob 0.1873933970928192\n",
      "iteration: 67 loss: -3.4739832878112793, same color prob 0.18114255368709564\n",
      "iteration: 68 loss: -3.483097553253174, same color prob 0.17969854176044464\n",
      "iteration: 69 loss: -3.4824390411376953, same color prob 0.1851307600736618\n",
      "iteration: 70 loss: -3.4825923442840576, same color prob 0.17793139815330505\n",
      "iteration: 71 loss: -3.484689474105835, same color prob 0.1689969301223755\n",
      "iteration: 72 loss: -3.4724788665771484, same color prob 0.17419444024562836\n",
      "iteration: 73 loss: -3.458718776702881, same color prob 0.1812923103570938\n",
      "iteration: 74 loss: -3.4793946743011475, same color prob 0.18038544058799744\n",
      "iteration: 75 loss: -3.4768779277801514, same color prob 0.18394996225833893\n",
      "iteration: 76 loss: -3.4975075721740723, same color prob 0.1822182685136795\n",
      "iteration: 77 loss: -3.480158567428589, same color prob 0.17600078880786896\n",
      "iteration: 78 loss: -3.477161169052124, same color prob 0.16888365149497986\n",
      "iteration: 79 loss: -3.488201856613159, same color prob 0.17192597687244415\n",
      "iteration: 80 loss: -3.4872756004333496, same color prob 0.18291659653186798\n",
      "iteration: 81 loss: -3.4750874042510986, same color prob 0.19443289935588837\n",
      "iteration: 82 loss: -3.4793105125427246, same color prob 0.20197631418704987\n",
      "iteration: 83 loss: -3.485978126525879, same color prob 0.1920257955789566\n",
      "iteration: 84 loss: -3.4641027450561523, same color prob 0.1768888682126999\n",
      "iteration: 85 loss: -3.4678478240966797, same color prob 0.16556847095489502\n",
      "iteration: 86 loss: -3.468468189239502, same color prob 0.16443896293640137\n",
      "iteration: 87 loss: -3.4816300868988037, same color prob 0.18090754747390747\n",
      "iteration: 88 loss: -3.4686267375946045, same color prob 0.20878523588180542\n",
      "iteration: 89 loss: -3.471389055252075, same color prob 0.21704398095607758\n",
      "iteration: 90 loss: -3.4791202545166016, same color prob 0.20504817366600037\n",
      "iteration: 91 loss: -3.472229480743408, same color prob 0.17625652253627777\n",
      "iteration: 92 loss: -3.482987403869629, same color prob 0.1583564579486847\n",
      "iteration: 93 loss: -3.4774551391601562, same color prob 0.16459819674491882\n",
      "iteration: 94 loss: -3.47743821144104, same color prob 0.18621833622455597\n",
      "iteration: 95 loss: -3.481825351715088, same color prob 0.2119707316160202\n",
      "iteration: 96 loss: -3.4864633083343506, same color prob 0.2215220332145691\n",
      "iteration: 97 loss: -3.460956573486328, same color prob 0.20476487278938293\n",
      "iteration: 98 loss: -3.482614040374756, same color prob 0.1711626648902893\n",
      "iteration: 99 loss: -3.492271900177002, same color prob 0.1647472381591797\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    probs = out.softmax(dim=1)\n",
    "    # print(probs[data.edge_index[0,0]], probs[data.edge_index[1,0]])\n",
    "    # loss = cosine_sim(probs, data.edge_index)\n",
    "    # loss = mse_loss(probs, data.edge_index)\n",
    "    loss = -ncut_loss(probs, data.edge_index)\n",
    "    # print(i, loss)\n",
    "    # loss = -ncut_loss(probs, data.edge_index) + 10e-9 * size_reg(probs, k_colors)\n",
    "    # print(size_reg(probs, k_colors))\n",
    "    # print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    probs = out.softmax(dim=1)\n",
    "    collide_prob = cosine_sim(probs, data.edge_index)\n",
    "\n",
    "    print(f\"iteration: {i} loss: {loss.item()}, same color prob {collide_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.2751,     0.1258,     0.3271,     0.2720],\n",
      "        [    0.0082,     0.9753,     0.0090,     0.0075],\n",
      "        [    0.3097,     0.0088,     0.3695,     0.3120],\n",
      "        [    0.3146,     0.0009,     0.3724,     0.3121],\n",
      "        [    0.3144,     0.0001,     0.3736,     0.3119],\n",
      "        [    0.3146,     0.0010,     0.3724,     0.3121],\n",
      "        [    0.0007,     0.9979,     0.0008,     0.0006],\n",
      "        [    0.3142,     0.0022,     0.3712,     0.3123],\n",
      "        [    0.2866,     0.0871,     0.3415,     0.2848],\n",
      "        [    0.3023,     0.0252,     0.3666,     0.3060]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(probs[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample colors \n",
    "- Sample colors from trained model\n",
    "- Sample colors from uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming to find K-path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d7732aa4cfd9c3b18076e27109bef33dbd2e3d77433bf8952439ad24bec827c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('k-path': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
