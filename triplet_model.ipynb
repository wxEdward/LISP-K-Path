{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.loader import DataLoader,RandomNodeSampler, NeighborLoader, NeighborSampler\n",
    "from torch_geometric.utils import structured_negative_sampling, to_dense_adj, erdos_renyi_graph\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(edge_list_file, node_features_file):\n",
    "    # edge_list, [n_edges, 2]\n",
    "    # node_features, [n_nodes, dim_features]\n",
    "    edges = np.load(edge_list_file)\n",
    "    assert edges.shape[1] == 2\n",
    "\n",
    "    features = np.load(node_features_file)\n",
    "    n_nodes = features.shape[0]\n",
    "    # print(features.shape)\n",
    "    assert edges.min() >= 0\n",
    "    assert edges.max() <= n_nodes-1\n",
    "    # print(edges.min())\n",
    "    print(f\"Number of edges: {edges.shape[0]}, number of nodes: {n_nodes}, feature dim: {features.shape[1]}\")\n",
    "    \n",
    "    data = Data(x = torch.from_numpy(features).float(), edge_index=torch.from_numpy(edges).t().contiguous())\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def generative_er_graph_deepwalk(num_nodes, edge_index):\n",
    "    # er_graph = erdos_renyi_graph(num_nodes=256, edge_prob=0.1, directed=False)\n",
    "    er_graph = edge_index\n",
    "    model = Node2Vec(er_graph, embedding_dim=128, walk_length=10,\n",
    "                     context_size=10, walks_per_node=10,\n",
    "                     num_negative_samples=1, p=1, q=1, sparse=True).to(device)\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    \n",
    "    for epoch in range(1, 101):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        train_loss = total_loss / len(loader)\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {train_loss:.4f}')\n",
    "    z = model(torch.arange(num_nodes, device=device))\n",
    "    return Data(x=z, edge_index=er_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 8959, number of nodes: 1870, feature dim: 128\n"
     ]
    }
   ],
   "source": [
    "_data = load_data(\"data/bio-yeast-protein/bio-yeast-protein-inter.edges_edge_list_mapped.npy\", \n",
    "\"data/bio-yeast-protein/bio-yeast-protein-inter.edges.embeddings_features.npy\")\n",
    "_data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 9.0841\n",
      "Epoch: 02, Loss: 7.6387\n",
      "Epoch: 03, Loss: 6.8107\n",
      "Epoch: 04, Loss: 6.2232\n",
      "Epoch: 05, Loss: 5.8814\n",
      "Epoch: 06, Loss: 5.5677\n",
      "Epoch: 07, Loss: 5.2296\n",
      "Epoch: 08, Loss: 5.0321\n",
      "Epoch: 09, Loss: 4.8172\n",
      "Epoch: 10, Loss: 4.5485\n",
      "Epoch: 11, Loss: 4.3611\n",
      "Epoch: 12, Loss: 4.1457\n",
      "Epoch: 13, Loss: 3.9777\n",
      "Epoch: 14, Loss: 3.8263\n",
      "Epoch: 15, Loss: 3.6091\n",
      "Epoch: 16, Loss: 3.4510\n",
      "Epoch: 17, Loss: 3.3191\n",
      "Epoch: 18, Loss: 3.1439\n",
      "Epoch: 19, Loss: 3.0125\n",
      "Epoch: 20, Loss: 2.9010\n",
      "Epoch: 21, Loss: 2.7806\n",
      "Epoch: 22, Loss: 2.6766\n",
      "Epoch: 23, Loss: 2.5406\n",
      "Epoch: 24, Loss: 2.4355\n",
      "Epoch: 25, Loss: 2.3520\n",
      "Epoch: 26, Loss: 2.2701\n",
      "Epoch: 27, Loss: 2.1848\n",
      "Epoch: 28, Loss: 2.0998\n",
      "Epoch: 29, Loss: 2.0315\n",
      "Epoch: 30, Loss: 1.9587\n",
      "Epoch: 31, Loss: 1.8836\n",
      "Epoch: 32, Loss: 1.8295\n",
      "Epoch: 33, Loss: 1.7620\n",
      "Epoch: 34, Loss: 1.7199\n",
      "Epoch: 35, Loss: 1.6574\n",
      "Epoch: 36, Loss: 1.6091\n",
      "Epoch: 37, Loss: 1.5666\n",
      "Epoch: 38, Loss: 1.5250\n",
      "Epoch: 39, Loss: 1.4836\n",
      "Epoch: 40, Loss: 1.4403\n",
      "Epoch: 41, Loss: 1.4168\n",
      "Epoch: 42, Loss: 1.3718\n",
      "Epoch: 43, Loss: 1.3368\n",
      "Epoch: 44, Loss: 1.3003\n",
      "Epoch: 45, Loss: 1.2765\n",
      "Epoch: 46, Loss: 1.2398\n",
      "Epoch: 47, Loss: 1.2265\n",
      "Epoch: 48, Loss: 1.2006\n",
      "Epoch: 49, Loss: 1.1833\n",
      "Epoch: 50, Loss: 1.1609\n",
      "Epoch: 51, Loss: 1.1316\n",
      "Epoch: 52, Loss: 1.1136\n",
      "Epoch: 53, Loss: 1.0921\n",
      "Epoch: 54, Loss: 1.0779\n",
      "Epoch: 55, Loss: 1.0677\n",
      "Epoch: 56, Loss: 1.0534\n",
      "Epoch: 57, Loss: 1.0339\n",
      "Epoch: 58, Loss: 1.0240\n",
      "Epoch: 59, Loss: 1.0122\n",
      "Epoch: 60, Loss: 1.0040\n",
      "Epoch: 61, Loss: 0.9931\n",
      "Epoch: 62, Loss: 0.9810\n",
      "Epoch: 63, Loss: 0.9684\n",
      "Epoch: 64, Loss: 0.9637\n",
      "Epoch: 65, Loss: 0.9570\n",
      "Epoch: 66, Loss: 0.9474\n",
      "Epoch: 67, Loss: 0.9400\n",
      "Epoch: 68, Loss: 0.9340\n",
      "Epoch: 69, Loss: 0.9303\n",
      "Epoch: 70, Loss: 0.9227\n",
      "Epoch: 71, Loss: 0.9134\n",
      "Epoch: 72, Loss: 0.9104\n",
      "Epoch: 73, Loss: 0.9094\n",
      "Epoch: 74, Loss: 0.8994\n",
      "Epoch: 75, Loss: 0.8978\n",
      "Epoch: 76, Loss: 0.8892\n",
      "Epoch: 77, Loss: 0.8889\n",
      "Epoch: 78, Loss: 0.8837\n",
      "Epoch: 79, Loss: 0.8827\n",
      "Epoch: 80, Loss: 0.8735\n",
      "Epoch: 81, Loss: 0.8717\n",
      "Epoch: 82, Loss: 0.8664\n",
      "Epoch: 83, Loss: 0.8682\n",
      "Epoch: 84, Loss: 0.8623\n",
      "Epoch: 85, Loss: 0.8609\n",
      "Epoch: 86, Loss: 0.8571\n",
      "Epoch: 87, Loss: 0.8546\n",
      "Epoch: 88, Loss: 0.8540\n",
      "Epoch: 89, Loss: 0.8514\n",
      "Epoch: 90, Loss: 0.8510\n",
      "Epoch: 91, Loss: 0.8495\n",
      "Epoch: 92, Loss: 0.8467\n",
      "Epoch: 93, Loss: 0.8416\n",
      "Epoch: 94, Loss: 0.8405\n",
      "Epoch: 95, Loss: 0.8392\n",
      "Epoch: 96, Loss: 0.8353\n",
      "Epoch: 97, Loss: 0.8360\n",
      "Epoch: 98, Loss: 0.8358\n",
      "Epoch: 99, Loss: 0.8346\n",
      "Epoch: 100, Loss: 0.8332\n"
     ]
    }
   ],
   "source": [
    "data = generative_er_graph_deepwalk(num_nodes=1870, edge_index=_data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.x = data.x.data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 8959, number of nodes: 1870, feature dim: 128\n"
     ]
    }
   ],
   "source": [
    "# data = load_data(\"data/PP/ppi_edge_list_mapped.npy\", \"data/PP/ppi_embeddings_features.npy\")\n",
    "\n",
    "# data = load_data(\"data/bio-WormNet/bio-WormNet-v3-benchmark.edges_edge_list_mapped.npy\", \n",
    "# \"data/bio-WormNet/bio-WormNet-v3-benchmark.edges.embeddings_features.npy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = data.x.shape[1]\n",
    "k_colors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.conv1 = GATConv(feat_dim, 64)\n",
    "        # self.conv2 = GATConv(64, 64)\n",
    "\n",
    "        self.conv1 = GCNConv(feat_dim, 64)\n",
    "        self.conv2 = GCNConv(64, 64)\n",
    "        # self.conv3 = GCNConv(512, 512)\n",
    "        # self.conv4 = GCNConv(32, 32)\n",
    "\n",
    "        self.lin1 = nn.Linear(64, 32)\n",
    "        self.lin2 = nn.Linear(32, out_dim)\n",
    "        # self.lin3 = nn.Linear(64, out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.conv4(x, edge_index)\n",
    "        # x = x.relu()\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        # x = x.relu()\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.lin3(x)\n",
    "        \n",
    "        return x # return logits instead of probs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1870, 128])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1870, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# test_net = Network(128, k_colors).to(device)\n",
    "\n",
    "# test_out = test_net(data.x, data.edge_index)\n",
    "# print(test_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_loss \n",
    "\n",
    "def inner_product_loss(network_out):\n",
    "    batch_size = network_out.shape[0]//2\n",
    "    a = network_out[:batch_size, :]\n",
    "    b = network_out[batch_size:, :]\n",
    "    mean_dot_prod = torch.mean(torch.sum(a * b, dim=1), dim=0)\n",
    "    # print(b.argmax(dim=1).shape)\n",
    "    # same_color = (a.argmax(dim=1) == b.argmax(dim=1)).int().sum()\n",
    "    # return same_color/a.shape[0]\n",
    "    return mean_dot_prod\n",
    "\n",
    "def negative_entropy_loss(network_out):\n",
    "    # num_nodes x k\n",
    "    log = torch.log(network_out)\n",
    "    prod = network_out * log \n",
    "    s = torch.sum(prod, dim=-1)\n",
    "    return s.mean()\n",
    "\n",
    "def percent_same_color(network_out):\n",
    "    # percent of pairs having same color\n",
    "    # [2 * batch_size, feature_size]\n",
    "    arg_max = torch.argmax(network_out, dim=1)\n",
    "    arg_max_reshaped = arg_max.reshape([2, -1])\n",
    "    result = torch.sum((arg_max_reshaped[0,:] == arg_max_reshaped[1,:]).int())/arg_max_reshaped.shape[1]\n",
    "    # print(result.shape)\n",
    "    return result\n",
    "    \n",
    "def assign_color_sampled_edges(sampled_edges, k):\n",
    "    # given a sampled batch of edges, assign random color to each end points\n",
    "    all_nodes = sampled_edges.ravel()\n",
    "    # print(sampled_edges.shape)\n",
    "    # colors = torch.randint(low=0, high=k, size=all_nodes.shape)\n",
    "    color_dict = {node.item():torch.randint(low=0, high=k, size=(1,)).item() for node in all_nodes}\n",
    "    count_same = 0\n",
    "    for i in range(sampled_edges.shape[1]):\n",
    "        assert (sampled_edges[0,i] in all_nodes)\n",
    "        assert (sampled_edges[1,i] in all_nodes)\n",
    "        if color_dict[sampled_edges[0,i].item()] == color_dict[sampled_edges[1,i].item()]:\n",
    "            count_same+=1\n",
    "    return count_same/sampled_edges.shape[1]\n",
    "\n",
    "# test = negative_entropy_loss(out)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# loader = NeighborLoader(data, num_neighbors=[10] * 2, shuffle=True, batch_size=10)\n",
    "\n",
    "\n",
    "model = Network(data.x.shape[1], k_colors).to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "\n",
    "cos_sim = torch.nn.CosineSimilarity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cosine_sim(node_probs, edge_index): \n",
    "    # assuming continous node numbering\n",
    "    # for each edge, get a loss term, then take mean\n",
    "    total_loss = 0.\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1_prob = node_probs[edge_index[0, i],:]\n",
    "        node2_prob = node_probs[edge_index[1, i],:]\n",
    "        total_loss += torch.dot(node1_prob, node2_prob)\n",
    "    return total_loss/edge_index.shape[1]\n",
    "\n",
    "def same_color_prob(node_probs, edge_index):\n",
    "    n_same_color = 0\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1_prob = node_probs[edge_index[0, i],:]\n",
    "        node2_prob = node_probs[edge_index[1, i],:]\n",
    "        # print(torch.argmax(node1_prob), torch.argmax(node2_prob))\n",
    "        if torch.argmax(node1_prob) == torch.argmax(node2_prob):\n",
    "            # print(node1_prob, node2_prob)\n",
    "            n_same_color+=1\n",
    "    return n_same_color/edge_index.shape[1]\n",
    "\n",
    "mse_criterion = torch.nn.MSELoss()\n",
    "\n",
    "def mse_loss(node_probs, edge_index):\n",
    "    total_loss = 0.\n",
    "    for i in range(edge_index.shape[1]):\n",
    "        node1_prob = node_probs[edge_index[0, i],:]\n",
    "        node2_prob = node_probs[edge_index[1, i],:]\n",
    "        total_loss += mse_criterion(node1_prob, node2_prob)\n",
    "    return total_loss/edge_index.shape[1]\n",
    "\n",
    "def ncut_loss(node_probs, edge_index):\n",
    "    Y = node_probs \n",
    "    A = torch.squeeze(to_dense_adj(edge_index, max_num_nodes=node_probs.shape[0]))\n",
    "    # print(A.shape, node_probs.shape)\n",
    "    assert (A.shape[0] == node_probs.shape[0])\n",
    "    \n",
    "    D = torch.sum(A, dim=1)\n",
    "    Gamma = Y.t()@D \n",
    "    sum_mat = torch.mul(torch.div(Y, Gamma)@((1-Y).t()), A)\n",
    "    # sum_mat = torch.mul(Y@((1-Y).t()), A)\n",
    "    # print(sum_mat)\n",
    "    ncut = torch.sum(sum_mat)\n",
    "\n",
    "    return ncut\n",
    "\n",
    "\n",
    "def test_ncut_loss():\n",
    "    # edge_index=torch.tensor([[0,1], [1, 0], [0, 3], [3, 0], [1, 2], [2, 1], [2, 3], [3, 2]])\n",
    "    edge_index = torch.tensor([[0, 1, 0, 3, 1, 2, 2, 3],[1, 0, 3, 0, 2, 1, 3, 2]])\n",
    "    # A = torch.tensor([\n",
    "    #     [0, 1, 0, 1],\n",
    "    #     [1, 0, 1, 0],\n",
    "    #     [0, 1, 0, 1],\n",
    "    #     [1, 0, 1, 0]\n",
    "    # ])\n",
    "    # print(edge_index)\n",
    "\n",
    "    A = torch.squeeze(to_dense_adj(edge_index, max_num_nodes=4))\n",
    "    # print(A)\n",
    "    Y = torch.tensor([\n",
    "        [0.5, 0.5],\n",
    "        [0.25, 0.75],\n",
    "        [0.75, 0.25],\n",
    "        [0.1, 0.9]\n",
    "    ])\n",
    "\n",
    "    D = torch.tensor([2., 2., 2., 2.])\n",
    "\n",
    "    Gamma = Y.t()@D\n",
    "    loss_matrix = ((Y/Gamma)@((1-Y).t()))*A\n",
    "    # loss_matrix = ((Y)@((1-Y).t()))*A\n",
    "    # print(loss_matrix)\n",
    "    correct_loss = loss_matrix.sum()\n",
    "\n",
    "    # assert (ncut_loss(Y, edge_index) == correct_loss)\n",
    "\n",
    "test_ncut_loss()\n",
    "\n",
    "def size_reg(node_probs, k_colors):\n",
    "    sizes = node_probs.sum(dim=1)\n",
    "    mean_size = node_probs.shape[0]/k_colors\n",
    "    size_reg = torch.dot(sizes-mean_size, sizes-mean_size)\n",
    "    return size_reg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 loss: -4.480984687805176, same color prob 0.1934989094734192\n",
      "iteration: 1 loss: -4.475341796875, same color prob 0.19673869013786316\n",
      "iteration: 2 loss: -4.459436416625977, same color prob 0.17213159799575806\n",
      "iteration: 3 loss: -4.474292755126953, same color prob 0.15670636296272278\n",
      "iteration: 4 loss: -4.468151092529297, same color prob 0.144439235329628\n",
      "iteration: 5 loss: -4.452651023864746, same color prob 0.14735068380832672\n",
      "iteration: 6 loss: -4.470206260681152, same color prob 0.1559072732925415\n",
      "iteration: 7 loss: -4.472042560577393, same color prob 0.16430611908435822\n",
      "iteration: 8 loss: -4.476047515869141, same color prob 0.18292781710624695\n",
      "iteration: 9 loss: -4.470345497131348, same color prob 0.18367737531661987\n",
      "iteration: 10 loss: -4.485378265380859, same color prob 0.17225442826747894\n",
      "iteration: 11 loss: -4.463540077209473, same color prob 0.1613338738679886\n",
      "iteration: 12 loss: -4.453773498535156, same color prob 0.15815061330795288\n",
      "iteration: 13 loss: -4.491358757019043, same color prob 0.15536490082740784\n",
      "iteration: 14 loss: -4.469290256500244, same color prob 0.16486327350139618\n",
      "iteration: 15 loss: -4.478306770324707, same color prob 0.15529298782348633\n",
      "iteration: 16 loss: -4.462177276611328, same color prob 0.15624751150608063\n",
      "iteration: 17 loss: -4.466930389404297, same color prob 0.16292278468608856\n",
      "iteration: 18 loss: -4.474580764770508, same color prob 0.164320170879364\n",
      "iteration: 19 loss: -4.470627784729004, same color prob 0.16555027663707733\n",
      "iteration: 20 loss: -4.452146530151367, same color prob 0.15910162031650543\n",
      "iteration: 21 loss: -4.472454071044922, same color prob 0.17084260284900665\n",
      "iteration: 22 loss: -4.491722106933594, same color prob 0.19642360508441925\n",
      "iteration: 23 loss: -4.466695785522461, same color prob 0.22432512044906616\n",
      "iteration: 24 loss: -4.454898834228516, same color prob 0.20157359540462494\n",
      "iteration: 25 loss: -4.46937894821167, same color prob 0.16533371806144714\n",
      "iteration: 26 loss: -4.471185207366943, same color prob 0.1506793349981308\n",
      "iteration: 27 loss: -4.472524642944336, same color prob 0.15670335292816162\n",
      "iteration: 28 loss: -4.479032516479492, same color prob 0.1848633885383606\n",
      "iteration: 29 loss: -4.475661277770996, same color prob 0.21248750388622284\n",
      "iteration: 30 loss: -4.460119247436523, same color prob 0.2162168323993683\n",
      "iteration: 31 loss: -4.470624923706055, same color prob 0.18977303802967072\n",
      "iteration: 32 loss: -4.482004165649414, same color prob 0.15787416696548462\n",
      "iteration: 33 loss: -4.476802825927734, same color prob 0.15427546203136444\n",
      "iteration: 34 loss: -4.472367763519287, same color prob 0.1646602302789688\n",
      "iteration: 35 loss: -4.460505962371826, same color prob 0.1882803738117218\n",
      "iteration: 36 loss: -4.480179786682129, same color prob 0.2121278941631317\n",
      "iteration: 37 loss: -4.475035190582275, same color prob 0.20735034346580505\n",
      "iteration: 38 loss: -4.471839904785156, same color prob 0.18092402815818787\n",
      "iteration: 39 loss: -4.477566719055176, same color prob 0.1651524305343628\n",
      "iteration: 40 loss: -4.4835615158081055, same color prob 0.15522818267345428\n",
      "iteration: 41 loss: -4.476248741149902, same color prob 0.16200092434883118\n",
      "iteration: 42 loss: -4.495989799499512, same color prob 0.1843644231557846\n",
      "iteration: 43 loss: -4.465939521789551, same color prob 0.18219774961471558\n",
      "iteration: 44 loss: -4.479516983032227, same color prob 0.17417234182357788\n",
      "iteration: 45 loss: -4.4801106452941895, same color prob 0.16764339804649353\n",
      "iteration: 46 loss: -4.4596052169799805, same color prob 0.16763634979724884\n",
      "iteration: 47 loss: -4.47559928894043, same color prob 0.17087869346141815\n",
      "iteration: 48 loss: -4.4867939949035645, same color prob 0.1734747439622879\n",
      "iteration: 49 loss: -4.490206718444824, same color prob 0.18509511649608612\n",
      "iteration: 50 loss: -4.4585862159729, same color prob 0.17152954638004303\n",
      "iteration: 51 loss: -4.486708641052246, same color prob 0.14973819255828857\n",
      "iteration: 52 loss: -4.465749740600586, same color prob 0.14514993131160736\n",
      "iteration: 53 loss: -4.467049598693848, same color prob 0.15896733105182648\n",
      "iteration: 54 loss: -4.484094619750977, same color prob 0.1727345585823059\n",
      "iteration: 55 loss: -4.463074684143066, same color prob 0.17742478847503662\n",
      "iteration: 56 loss: -4.47247838973999, same color prob 0.17058609426021576\n",
      "iteration: 57 loss: -4.491142272949219, same color prob 0.15379074215888977\n",
      "iteration: 58 loss: -4.489877700805664, same color prob 0.1438705176115036\n",
      "iteration: 59 loss: -4.465214729309082, same color prob 0.15579788386821747\n",
      "iteration: 60 loss: -4.488991737365723, same color prob 0.17509770393371582\n",
      "iteration: 61 loss: -4.493898868560791, same color prob 0.18651941418647766\n",
      "iteration: 62 loss: -4.48953914642334, same color prob 0.19671903550624847\n",
      "iteration: 63 loss: -4.485033988952637, same color prob 0.17484639585018158\n",
      "iteration: 64 loss: -4.4829182624816895, same color prob 0.15974970161914825\n",
      "iteration: 65 loss: -4.4729461669921875, same color prob 0.157237708568573\n",
      "iteration: 66 loss: -4.474730968475342, same color prob 0.16960623860359192\n",
      "iteration: 67 loss: -4.477215766906738, same color prob 0.18898464739322662\n",
      "iteration: 68 loss: -4.46718692779541, same color prob 0.20136535167694092\n",
      "iteration: 69 loss: -4.46974515914917, same color prob 0.17961204051971436\n",
      "iteration: 70 loss: -4.477025032043457, same color prob 0.16025447845458984\n",
      "iteration: 71 loss: -4.483671188354492, same color prob 0.1413157731294632\n",
      "iteration: 72 loss: -4.476592540740967, same color prob 0.14654435217380524\n",
      "iteration: 73 loss: -4.462765693664551, same color prob 0.18590614199638367\n",
      "iteration: 74 loss: -4.49531364440918, same color prob 0.22769811749458313\n",
      "iteration: 75 loss: -4.466640949249268, same color prob 0.2665436565876007\n",
      "iteration: 76 loss: -4.436216831207275, same color prob 0.22427384555339813\n",
      "iteration: 77 loss: -4.46207332611084, same color prob 0.16918641328811646\n",
      "iteration: 78 loss: -4.470808982849121, same color prob 0.14920669794082642\n",
      "iteration: 79 loss: -4.463156700134277, same color prob 0.14405351877212524\n",
      "iteration: 80 loss: -4.478782653808594, same color prob 0.15638890862464905\n",
      "iteration: 81 loss: -4.485652923583984, same color prob 0.18717102706432343\n",
      "iteration: 82 loss: -4.467512130737305, same color prob 0.20730410516262054\n",
      "iteration: 83 loss: -4.47027063369751, same color prob 0.21844913065433502\n",
      "iteration: 84 loss: -4.457777976989746, same color prob 0.20040105283260345\n",
      "iteration: 85 loss: -4.469597339630127, same color prob 0.16936956346035004\n",
      "iteration: 86 loss: -4.473285675048828, same color prob 0.14948534965515137\n",
      "iteration: 87 loss: -4.476943492889404, same color prob 0.16067664325237274\n",
      "iteration: 88 loss: -4.4840850830078125, same color prob 0.1897207796573639\n",
      "iteration: 89 loss: -4.486268997192383, same color prob 0.23011037707328796\n",
      "iteration: 90 loss: -4.47019624710083, same color prob 0.21153852343559265\n",
      "iteration: 91 loss: -4.469165325164795, same color prob 0.1614968329668045\n",
      "iteration: 92 loss: -4.478862762451172, same color prob 0.1404641717672348\n",
      "iteration: 93 loss: -4.454888820648193, same color prob 0.1426955908536911\n",
      "iteration: 94 loss: -4.486148834228516, same color prob 0.1630176603794098\n",
      "iteration: 95 loss: -4.4732513427734375, same color prob 0.1903706043958664\n",
      "iteration: 96 loss: -4.469158172607422, same color prob 0.20658062398433685\n",
      "iteration: 97 loss: -4.46981143951416, same color prob 0.1826961189508438\n",
      "iteration: 98 loss: -4.475182056427002, same color prob 0.16728442907333374\n",
      "iteration: 99 loss: -4.4861884117126465, same color prob 0.1534833312034607\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    model.train()\n",
    "    out = model(_data.x, _data.edge_index)\n",
    "    probs = out.softmax(dim=1)\n",
    "    # print(probs[data.edge_index[0,0]], probs[data.edge_index[1,0]])\n",
    "    # loss = cosine_sim(probs, data.edge_index)\n",
    "    # loss = mse_loss(probs, data.edge_index)\n",
    "    loss = -ncut_loss(probs, _data.edge_index)\n",
    "    # print(i, loss)\n",
    "    # loss = -ncut_loss(probs, data.edge_index) + 10e-9 * size_reg(probs, k_colors)\n",
    "    # print(size_reg(probs, k_colors))\n",
    "    # print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # testing\n",
    "    model.eval()\n",
    "    # # edges_chosen = data.edge_index[:,:data.edge_index.shape[1]//3]\n",
    "    # edges_chosen = data.edge_index\n",
    "    out = model(_data.x, _data.edge_index)\n",
    "    probs = out.softmax(dim=1)\n",
    "    collide_prob = cosine_sim(probs, data.edge_index)\n",
    "\n",
    "    print(f\"iteration: {i} loss: {loss.item()}, same color prob {collide_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_probs = probs.detach().to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(-ncut_loss(probs, data.edge_index))\n",
    "# size_reg(probs, k_colors)*1e-9\n",
    "np.save('bio-yeast-protein_color_probs', np_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8535"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8959\n",
      "0\n",
      "0.5023680925369263 0.5023680925369263 0.5259515570934256\n",
      "1\n",
      "0.5000861287117004 0.5000861287117004 0.5018417234066302\n",
      "2\n",
      "0.5001647472381592 0.5001647472381592 0.5005022882018082\n",
      "3\n",
      "0.5003552436828613 0.5003552436828613 0.512557205045206\n",
      "4\n",
      "0.50022292137146 0.50022292137146 0.5182498046656993\n",
      "5\n",
      "0.5000616908073425 0.5000616908073425 0.4875544145551959\n",
      "6\n",
      "0.4999978244304657 0.4999978244304657 0.5093202366335529\n",
      "7\n",
      "0.5000165700912476 0.5000165700912476 0.5035160174126576\n",
      "8\n",
      "0.5000544190406799 0.5000544190406799 0.5138966402500279\n",
      "9\n",
      "0.5000529885292053 0.5000529885292053 0.5214867730773524\n"
     ]
    }
   ],
   "source": [
    "# import random\n",
    "\n",
    "# num_edges = data.edge_index.shape[1]\n",
    "# # batch_size = 20000\n",
    "# # batch_size = data.x.shape[0]\n",
    "# batch_size = data.edge_index.shape[1]\n",
    "# print(batch_size)\n",
    "# # batch_size = 1000\n",
    "# num_batches = num_edges//batch_size + 1\n",
    "# index_list = list(range(num_edges))\n",
    "# alpha = 1.\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(10):\n",
    "#     random.shuffle(index_list)\n",
    "#     print(epoch)\n",
    "#     # for batch_index in range(num_batches):\n",
    "#     for batch_index in range(1):\n",
    "#         sampled_edge_index = index_list[batch_size*batch_index : min(batch_size*(batch_index+1), num_edges)]\n",
    "#         # print(len(sampled_edge_index))\n",
    "#         # print(min(batch_size*(batch_index+1), num_edges))\n",
    "#         sampled_edges = data.edge_index[:, sampled_edge_index]\n",
    "#         uniform_random_ratio = assign_color_sampled_edges(sampled_edges, k_colors)\n",
    "#         # if batch_index == 10:\n",
    "#         sampled_nodes = sampled_edges.ravel()\n",
    "#         # print(sampled_nodes.device)\n",
    "\n",
    "#         # print(sampled_nodes.shape)\n",
    "#         # print(sampled_nodes.dtype)\n",
    "#         _, n_id, adjs = sampler.sample(sampled_nodes.to('cpu'))\n",
    "#         # print(n_id.shape)\n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data.x[n_id], adjs)\n",
    "\n",
    "#         current_batch_size = out.shape[0]//2\n",
    "#         # print(current_batch_size)\n",
    "#             # print('======')\n",
    "#         # print(out[0,:])\n",
    "#         # print(out[current_batch_size,:])\n",
    "#         # print(current_batch_size.dtype)\n",
    "#         # loss = -criterion(out[:current_batch_size,:], out[current_batch_size:, :]) \n",
    "#         # loss = -cos_sim(out[:current_batch_size,:], out[current_batch_size:, :]).mean() \n",
    "        \n",
    "#         # loss = -criterion(out[:batch_size,:], out[batch_size:, :]) + alpha * negative_entropy_loss(out)\n",
    "#         # loss = inner_product_loss(out) - alpha * negative_entropy_loss(out)\n",
    "#         loss = inner_product_loss(out)\n",
    "#         # print(loss.item(), percent_same_color(out).item(), uniform_random_ratio)\n",
    "#         print(loss.item(), inner_product_loss(out).item(), uniform_random_ratio)\n",
    "#         # print(loss.item(), inner_product_loss(out).item())\n",
    "#         # print()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "margin = 0.01\n",
    "train_running_loss = 0.0\n",
    "for sample in loader:\n",
    "    # print(device)\n",
    "    sample.x = sample.x.to(device).to(dtype=torch.float32)\n",
    "    sample.x.requires_grad=True\n",
    "    # sample.x = requires_grad=True\n",
    "    sample.edge_index = sample.edge_index.to(device).long()\n",
    "    # print(sample.x.is_cuda)\n",
    "    # print(type(sample.edge_index))\n",
    "    # print(sample.edge_index.is_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    i, j, k = structured_negative_sampling(sample.edge_index)\n",
    "    negatives = (i,k)   #not neighbors\n",
    "    positives = (i,j)   #neighbors \n",
    "    output = model(sample)\n",
    "    #pos = model.similarity(sample.x[i], sample.x[j])\n",
    "    #neg = model.similarity(sample.x[i], sample.x[k])\n",
    "    pos = model.similarity(output[i], output[j])\n",
    "    neg = model.similarity(output[i], output[k])\n",
    "    diff =pos.diag() -neg.diag() +margin      # Note for coloring, we want negatives closer and positives further\n",
    "    triplet_loss_matrix = diff.mean()\n",
    "    loss = triplet_loss_matrix\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_running_loss += loss.detach().item()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d7732aa4cfd9c3b18076e27109bef33dbd2e3d77433bf8952439ad24bec827c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('k-path': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
