{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch_geometric.nn import GCNConv, SAGEConv \n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.loader import DataLoader,RandomNodeSampler, NeighborLoader, NeighborSampler\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(edge_list_file, node_features_file):\n",
    "    # edge_list, [n_edges, 2]\n",
    "    # node_features, [n_nodes, dim_features]\n",
    "    edges = np.load(edge_list_file)\n",
    "    assert edges.shape[1] == 2\n",
    "\n",
    "    features = np.load(node_features_file)\n",
    "    n_nodes = features.shape[0]\n",
    "    \n",
    "    assert edges.min() >= 0\n",
    "    assert edges.max() <= n_nodes-1\n",
    "    \n",
    "    print(f\"Number of edges: {edges.shape[0]}, number of nodes: {n_nodes}, feature dim: {features.shape[1]}\")\n",
    "    \n",
    "    data = Data(x = torch.from_numpy(features).float(), edge_index=torch.from_numpy(edges).t().contiguous())\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges: 8959, number of nodes: 1870, feature dim: 128\n"
     ]
    }
   ],
   "source": [
    "# data = load_data(\"data/PP/ppi_edge_list_mapped.npy\", \"data/PP/ppi_embeddings_features.npy\")\n",
    "data = load_data(\"data/bio-yeast-protein/bio-yeast-protein-inter.edges_edge_list_mapped.npy\", \n",
    "\"data/bio-yeast-protein/bio-yeast-protein-inter.edges.embeddings_features.npy\")\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dim = data.x.shape[1]\n",
    "k_colors = 2\n",
    "graph_layers_sizes = [feat_dim, 64, 64, 64]\n",
    "mlp_sizes = [64, 64, 32, k_colors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, gl_sizes, ll_sizes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gl_sizes = gl_sizes\n",
    "        self.ll_sizes = ll_sizes\n",
    "        self.num_conv_layers = len(gl_sizes)-1\n",
    "        self.num_lin_layers = len(ll_sizes)-1\n",
    "\n",
    "        self.convs = torch.nn.ModuleList([SAGEConv(gl_sizes[i], gl_sizes[i+1]) for i in range(self.num_conv_layers)])\n",
    "        self.mlp = torch.nn.ModuleList([nn.Linear(ll_sizes[i], ll_sizes[i+1]) for i in range(self.num_lin_layers)])\n",
    "\n",
    "    def forward(self, x, adjs):\n",
    "        # x node features of batch\n",
    "        # adjs, sampled bipartite graph\n",
    "\n",
    "        for i, (edge_index, _, size) in enumerate(adjs):\n",
    "            x_target = x[:size[1]]\n",
    "            edge_index = edge_index.to(device)\n",
    "            x = self.convs[i]((x, x_target), edge_index)\n",
    "            if i < self.num_conv_layers-1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)  \n",
    "        \n",
    "        assert x.shape == (x.shape[0], self.gl_sizes[-1])\n",
    "\n",
    "        # apply mlp for each node \n",
    "        for i, layer in enumerate(self.mlp):\n",
    "            x = layer(x)\n",
    "            if i < self.num_lin_layers-1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training) \n",
    "        \n",
    "        # x: [num_nodes, k]\n",
    "\n",
    "        assert x.shape == (x.shape[0], self.ll_sizes[-1])\n",
    "        \n",
    "        return x.softmax(dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SAGEConv(128, 64)\n",
      "  (1): SAGEConv(64, 64)\n",
      "  (2): SAGEConv(64, 64)\n",
      ")\n",
      "[128, 64, 64, 64]\n",
      "ModuleList(\n",
      "  (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (2): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n",
      "[64, 64, 32, 2]\n",
      "tensor([[0.4576, 0.5424],\n",
      "        [0.5058, 0.4942]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "sampler = NeighborSampler(\n",
    "    edge_index=data.edge_index,\n",
    "    # sizes = [15, 15, 15] # same as number of graph layers \n",
    "    sizes = [-1, -1, -1]\n",
    ")\n",
    "\n",
    "\n",
    "test_net = Network(graph_layers_sizes, mlp_sizes).to(device)\n",
    "print(test_net.convs)\n",
    "print(test_net.gl_sizes)\n",
    "print(test_net.mlp)\n",
    "print(test_net.ll_sizes)\n",
    "\n",
    "_, _n_id, _adjs = sampler.sample([1,2])\n",
    "test_out = test_net(data.x[_n_id], _adjs)\n",
    "print(test_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cosine_loss \n",
    "\n",
    "def inner_product_loss(network_out):\n",
    "    batch_size = network_out.shape[0]//2\n",
    "    a = network_out[:batch_size, :]\n",
    "    b = network_out[batch_size:, :]\n",
    "    mean_dot_prod = torch.mean(torch.sum(a * b, dim=1), dim=0)\n",
    "    # print(b.argmax(dim=1).shape)\n",
    "    # same_color = (a.argmax(dim=1) == b.argmax(dim=1)).int().sum()\n",
    "    # return same_color/a.shape[0]\n",
    "    return mean_dot_prod\n",
    "\n",
    "def negative_entropy_loss(network_out):\n",
    "    # num_nodes x k\n",
    "    log = torch.log(network_out)\n",
    "    prod = network_out * log \n",
    "    s = torch.sum(prod, dim=-1)\n",
    "    return s.mean()\n",
    "\n",
    "def percent_same_color(network_out):\n",
    "    # percent of pairs having same color\n",
    "    # [2 * batch_size, feature_size]\n",
    "    arg_max = torch.argmax(network_out, dim=1)\n",
    "    arg_max_reshaped = arg_max.reshape([2, -1])\n",
    "    result = torch.sum((arg_max_reshaped[0,:] == arg_max_reshaped[1,:]).int())/arg_max_reshaped.shape[1]\n",
    "    # print(result.shape)\n",
    "    return result\n",
    "    \n",
    "def assign_color_sampled_edges(sampled_edges, k):\n",
    "    # given a sampled batch of edges, assign random color to each end points\n",
    "    all_nodes = sampled_edges.ravel()\n",
    "    # print(sampled_edges.shape)\n",
    "    # colors = torch.randint(low=0, high=k, size=all_nodes.shape)\n",
    "    color_dict = {node.item():torch.randint(low=0, high=k, size=(1,)).item() for node in all_nodes}\n",
    "    count_same = 0\n",
    "    for i in range(sampled_edges.shape[1]):\n",
    "        assert (sampled_edges[0,i] in all_nodes)\n",
    "        assert (sampled_edges[1,i] in all_nodes)\n",
    "        if color_dict[sampled_edges[0,i].item()] == color_dict[sampled_edges[1,i].item()]:\n",
    "            count_same+=1\n",
    "    return count_same/sampled_edges.shape[1]\n",
    "\n",
    "# test = negative_entropy_loss(out)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# loader = NeighborLoader(data, num_neighbors=[10] * 2, shuffle=True, batch_size=10)\n",
    "\n",
    "\n",
    "model = Network(graph_layers_sizes, mlp_sizes).to(device)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=5e-4)\n",
    "\n",
    "cos_sim = torch.nn.CosineSimilarity()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8959\n",
      "0\n",
      "0.5023680925369263 0.5023680925369263 0.5259515570934256\n",
      "1\n",
      "0.5000861287117004 0.5000861287117004 0.5018417234066302\n",
      "2\n",
      "0.5001647472381592 0.5001647472381592 0.5005022882018082\n",
      "3\n",
      "0.5003552436828613 0.5003552436828613 0.512557205045206\n",
      "4\n",
      "0.50022292137146 0.50022292137146 0.5182498046656993\n",
      "5\n",
      "0.5000616908073425 0.5000616908073425 0.4875544145551959\n",
      "6\n",
      "0.4999978244304657 0.4999978244304657 0.5093202366335529\n",
      "7\n",
      "0.5000165700912476 0.5000165700912476 0.5035160174126576\n",
      "8\n",
      "0.5000544190406799 0.5000544190406799 0.5138966402500279\n",
      "9\n",
      "0.5000529885292053 0.5000529885292053 0.5214867730773524\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "num_edges = data.edge_index.shape[1]\n",
    "# batch_size = 20000\n",
    "# batch_size = data.x.shape[0]\n",
    "batch_size = data.edge_index.shape[1]\n",
    "print(batch_size)\n",
    "# batch_size = 1000\n",
    "num_batches = num_edges//batch_size + 1\n",
    "index_list = list(range(num_edges))\n",
    "alpha = 1.\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    random.shuffle(index_list)\n",
    "    print(epoch)\n",
    "    # for batch_index in range(num_batches):\n",
    "    for batch_index in range(1):\n",
    "        sampled_edge_index = index_list[batch_size*batch_index : min(batch_size*(batch_index+1), num_edges)]\n",
    "        # print(len(sampled_edge_index))\n",
    "        # print(min(batch_size*(batch_index+1), num_edges))\n",
    "        sampled_edges = data.edge_index[:, sampled_edge_index]\n",
    "        uniform_random_ratio = assign_color_sampled_edges(sampled_edges, k_colors)\n",
    "        # if batch_index == 10:\n",
    "        sampled_nodes = sampled_edges.ravel()\n",
    "        # print(sampled_nodes.device)\n",
    "\n",
    "        # print(sampled_nodes.shape)\n",
    "        # print(sampled_nodes.dtype)\n",
    "        _, n_id, adjs = sampler.sample(sampled_nodes.to('cpu'))\n",
    "        # print(n_id.shape)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x[n_id], adjs)\n",
    "\n",
    "        current_batch_size = out.shape[0]//2\n",
    "        # print(current_batch_size)\n",
    "            # print('======')\n",
    "        # print(out[0,:])\n",
    "        # print(out[current_batch_size,:])\n",
    "        # print(current_batch_size.dtype)\n",
    "        # loss = -criterion(out[:current_batch_size,:], out[current_batch_size:, :]) \n",
    "        # loss = -cos_sim(out[:current_batch_size,:], out[current_batch_size:, :]).mean() \n",
    "        \n",
    "        # loss = -criterion(out[:batch_size,:], out[batch_size:, :]) + alpha * negative_entropy_loss(out)\n",
    "        # loss = inner_product_loss(out) - alpha * negative_entropy_loss(out)\n",
    "        loss = inner_product_loss(out)\n",
    "        # print(loss.item(), percent_same_color(out).item(), uniform_random_ratio)\n",
    "        print(loss.item(), inner_product_loss(out).item(), uniform_random_ratio)\n",
    "        # print(loss.item(), inner_product_loss(out).item())\n",
    "        # print()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "margin = 0.01\n",
    "train_running_loss = 0.0\n",
    "for sample in loader:\n",
    "    # print(device)\n",
    "    sample.x = sample.x.to(device).to(dtype=torch.float32)\n",
    "    sample.x.requires_grad=True\n",
    "    # sample.x = requires_grad=True\n",
    "    sample.edge_index = sample.edge_index.to(device).long()\n",
    "    # print(sample.x.is_cuda)\n",
    "    # print(type(sample.edge_index))\n",
    "    # print(sample.edge_index.is_cuda)\n",
    "    optimizer.zero_grad()\n",
    "    i, j, k = structured_negative_sampling(sample.edge_index)\n",
    "    negatives = (i,k)   #not neighbors\n",
    "    positives = (i,j)   #neighbors \n",
    "    output = model(sample)\n",
    "    #pos = model.similarity(sample.x[i], sample.x[j])\n",
    "    #neg = model.similarity(sample.x[i], sample.x[k])\n",
    "    pos = model.similarity(output[i], output[j])\n",
    "    neg = model.similarity(output[i], output[k])\n",
    "    diff =pos.diag() -neg.diag() +margin      # Note for coloring, we want negatives closer and positives further\n",
    "    triplet_loss_matrix = diff.mean()\n",
    "    loss = triplet_loss_matrix\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_running_loss += loss.detach().item()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d7732aa4cfd9c3b18076e27109bef33dbd2e3d77433bf8952439ad24bec827c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('k-path': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
